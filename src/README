dgropp




=============================
=      File description     =
=============================
- SimpleHashSet -
    A superclass for implementations of hash-sets implementing the SimpleSet interface.
- OpenHashSet -
    A hash-set based on chaining. Extends SimpleHashSet. Note: the capacity of a chaining based hash-set is simply the
    number of buckets (the length of the array of lists).
- ClosedHashSet -
    A hash-set based on closed-hashing with quadratic probing. Extends SimpleHashSet.
- CollectionFacadeSet -
    Wraps an underlying Collection and serves to both simplify its API and give it a common type with the implemented
    SimpleHashSets.
- LinkedListContainer -
    A wrapper class that holds LinkedList<String> and delegates methods to it, to have an array of that class instead
    of LinkedList[] array (which is illegal in Java).
- SimpleSetPerformanceAnalyzer -
    Has a main method that measures the run-times requested in the "Performance Analysis" section.



=============================
=          Design           =
=============================
- Inheritance -
    SimpleHashSet contains methods that avoid code repetition in OpenHashSet and ClosedHashSet, while when necessary
    Overriding the parent method to add another functionality.
- Abstraction -
    The parent class is abstract (given in the API), as there is no implementation of this hash set. Also in this class
    I implemented abstract methods to be implemented in child classes, and thus maintaining a general guidlines of this
    behaviour (mostly resize related methods).
- Extensibility -
    By implementing resize related methods and abstract methods in the parent class, allowing for additional String hash
    sets to be created based on SimpleHashSet, and still follow the same rules and guidelines.
- Minimal API -
    All classes members that were not pre-defined are private. All methods that were not pre-defined in the given API
    are private when optional (helper methods and methods to be used by child class), and protected when necessary
    (mostly resize related) to share between child classes and create abstract methods to be implemented by child
    classes.
- Single responsibility -
    I implemented a LinkedList "bucket" class to use in the OpenHashSet, and defined in it methods to handle the bucket.
    Thus, allowing for replacing this bucket with another if necessary.



=============================
=  Implementation details   =
=============================
- Implementing OpenHashSet table -
    I implemented a wrapper class that holds LinkedList buckets, called LinkedListContainer. In OpenHashSet, I assigned
    the table to be an array of LinkedListContainer. Each added item is clamped using (hashCode & capacity), and if not
    existent already constructs a new LinkedListContainer bucket or adds to existent bucket. For each addition/deletion
    the element counter is updated, thus controlling set size very easily.
    The wrapper class is constructed with 1 element, meaning there is no scenario where the table array holds an empty
    bucket - buckets that are empty are null.
    In the wrapper class I implemented 'add' method to add items, 'contains' to search for item in the bucket (didn't
    use LinkedList 'contains' method, as it appears to be slower than just iterating over the list, as the buckets are
    normally low in size), 'find and delete' method to find item in bucket and remove it if found.
- Implementing ClosedHashSet delete mechanism -
    I initialized a primitive boolean array the size of the table capacity, that holds deleted indexes to ignore later
    on. On each deletion, the index that was deleted is marked as 'true' in the ignore list, thus when trying to find
    item in table, method will ignore this index and continue searching, and not stop once reached null index. When
    adding item, if found null index that is on the ignore list, fills this index and removes it from the ignore list.
    Also, on each resize the ignore list is reinitialized with the new table capacity.
    This is the most efficient solution I found to handle delete issues. At first, on each deletion I re-hashed the
    whole table which was very time consuming. I also revoked the use of flagging the deleted index, as that would limit
    the table's functionality, as it will limit certain values additions. Using TreeSet or any other permitted
    Collection was not optimal as it would cause run-time issues on each addition (to check if index is contained in
    Collection). Thus, I decided on a primitive array, as it is the most space efficient way I found to use, and the
    run-time for each check is O(1).
- Shared resize methods in SimpleHashSet -
    The resize method appeared to be quite similar in both sets, so to avoid repeating code I implemented general
    protected resize method in SimpleHashSet, and for the processes that required different code, I created abstract
    methods to be implemented in the child class. Each resize the table is reinitialized and all elements are re-hashed.
    To do so I copy the table elements (ignoring null indexes) to a temporary array and then re-hash all elements back
    to the new table, without searching for duplicates, as elements are certain to be unique. The new capacity in each
    resize is calculated as power of 2.
    In cases were a resize was needed on "add" method, it is expected to resize *before* the addition of the new item.
    To deal with that I first gathered if the new item is indeed unique and should be added, later I resized the table
    and on the previous elements table I added the new item, thus (re)hashing it with the rest of the previous elements.
- SimpleSetPerformanceAnalyzer -
    To simplify the performance analyzer class, I created a UI, were the user enters which tests he/she wants to
    perform. Code-wise, I divided each test to its own method. The UI receives an input, goes to a switch statement and
    performs all the relevant tests. The main method calls only 1 method, that prints the initial UI.



=============================
=    Answers to questions   =
=============================
- Account, in separate, for OpenHashSet’s and ClosedHashSet’s bad results for data1.txt -
    OpenHashSet holds a LinkedList bucket for each hash. In my implementation, once the hash found it goes to the
    LinkedList container and iterates until it finds if the item is indeed not there, and then adds it. When all the
    elements have the same hashcode it means basically that all the items go to the same index and thus each addition
    costs O(n).
    ClosedHashSet uses an Array that holds the hashcode clamped to it indexes. When all elements have the same hashcode
    it means that each addition starts on the same index as the one before it, and travels the same journey in the
    quadratic probing. Each new element will pass all the elements that were added before it. Meaning that each addition
     will cause an iteration on the entire array, thus still resulting in O(n).
- Summarize the strengths and weaknesses of each of the data structures as reflected by the results.
  Which would you use for which purposes? -
    - OpenHashSet and ClosedHashSet both performed quite well when receiving more natural mixture of elements (as data2).
    Also they performed very well when searching based on merely hashing. When dealing with items with the same
    hashcode, both contains and add were extremely slow.
    - Java TreeSet appears to have good average results on all issues - rarely the fastest, never the slowest.
    - Java LinkedList on the other hand performed relatively bad. In the API we were instructed to insist on unique
    elements also for non-unique elements collections like LinkedList. Also the "contains" method in LinkedList is very
    slow. Thus adding data was delayed with the contains method, and resulting in very slow performance on all tests.
    - Java HashSet is very fast, and mostly got the best results. Sometimes its results were too good...
        I would choose Java HashSet if I need to store unique elements and check if they exists.
- How did your two implementations compare between themselves? -
    - ClosedHashSet appears to perform better when the item it searches for exists in the table, as its journey towards
    an existing item is relatively shorter than for non-existent item, as then it would iterate over most of the array.
    - OpenHashSet appears to perform better when the item it searches for doesn't exist in the table. Also, when the
     element searched had a different hashcode, this set was extremely quick as a different hashcode means an empty
     LinkedList bucket.
- How did your implementations compare to Java’s built in HashSet? -
    Again, when dealing with elements with the same hashcode, Java HashSet was extremely quick in both add and contains
    when my implementations were *extremely* slow.
        - Add 'data1': OpenHashSet 55,215 ms. / ClosedHashSet 128,464 ms. / Java HashSet 120 ms.
        - Contains for item with the same hashcode in 'data1': OpenHashSet 933,331 ns. / ClosedHashSet 3,143,190(!) ns.
          / Java HashSet 39 ns.
    When dealing with a more natural mixture of elements, my implementations worked very much alike Java HashSet, and on
    times even better! Also when the elements had the same hashcode, and the "contains" method searched for an item with
    different hashcode, my implementations were quicker.
        - Add 'data2': OpenHashSet 52 ms. / ClosedHashSet 65 ms. / Java HashSet 39 ms.
        - Contains for item with different hashcode in 'data1': OpenHashSet 26 ns. / ClosedHashSet 61 ns.
          / Java HashSet 61 ns.
        - Contains for existing item in 'data2': OpenHashSet 133 ns. / ClosedHashSet 61 ns. / Java HashSet 68 ns.
        - Contains for non-existing item in 'data2': OpenHashSet 31 ns. / ClosedHashSet 76 ns. / Java HashSet 15 ns.
